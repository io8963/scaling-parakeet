# autobuild.py

import os
import shutil
import glob
import hashlib
from typing import List, Dict, Any
from collections import defaultdict

import config
from parser import get_metadata_and_content
import generator

# --- æ£€æŸ¥ä¾èµ– ---
try:
    import pygments
except ImportError:
    print("!!!! WARNING: Pygments not found. Code highlighting will be disabled. !!!!")


def hash_file(filepath: str) -> str:
    """è®¡ç®—æ–‡ä»¶çš„ SHA256 å“ˆå¸Œå€¼çš„å‰8ä½"""
    hasher = hashlib.sha256()
    try:
        with open(filepath, 'rb') as f:
            buf = f.read()
            hasher.update(buf)
        return hasher.hexdigest()[:8]
    except FileNotFoundError:
        return 'nohash'


def build_site():
    print("\n========================================")
    print("   ğŸš€ Starting Fresh Build Process")
    print("========================================\n")
    
    # -------------------------------------------------------------
    # 1. æš´åŠ›æ¸…ç† (Aggressive Clean)
    # -------------------------------------------------------------
    # Cloudflare ç¯å¢ƒä¸­æœ‰æ—¶ä¼šä¿ç•™ç¼“å­˜ï¼Œè¿™é‡Œæˆ‘ä»¬å¼ºåˆ¶åˆ é™¤æ•´ä¸ªæ„å»ºç›®å½•
    # ç¡®ä¿æ²¡æœ‰ä»»ä½•â€œåƒµå°¸â€æ–‡ä»¶æ®‹ç•™ã€‚
    # -------------------------------------------------------------
    print("--- 1. Cleaning Workspace ---")
    
    if os.path.exists(config.BUILD_DIR):
        print(f"   [Clean] Removing entire build directory: {config.BUILD_DIR}")
        try:
            shutil.rmtree(config.BUILD_DIR)
        except Exception as e:
            print(f"   [Error] Failed to clean build dir: {e}")
            # å¦‚æœåˆ é™¤å¤±è´¥ï¼ˆæå°‘è§ï¼‰ï¼Œå°è¯•æ‰‹åŠ¨æ¸…ç©ºå†…å®¹
            for item in os.listdir(config.BUILD_DIR):
                path = os.path.join(config.BUILD_DIR, item)
                if item not in ['.git', 'CNAME']: # ä¿æŠ¤ GitHub Pages ç›¸å…³æ–‡ä»¶
                    if os.path.isdir(path): shutil.rmtree(path, ignore_errors=True)
                    else: os.remove(path)
    
    # é‡æ–°åˆ›å»ºç©ºç›®å½•
    os.makedirs(config.BUILD_DIR, exist_ok=True)
    os.makedirs(config.POSTS_OUTPUT_DIR, exist_ok=True)
    os.makedirs(config.TAGS_OUTPUT_DIR, exist_ok=True)
    os.makedirs(config.STATIC_OUTPUT_DIR, exist_ok=True) 
    
    print("   [Init] Build directories created.")

    # -------------------------------------------------------------
    # 2. èµ„æºå¤„ç† (CSS Hash)
    # -------------------------------------------------------------
    print("\n--- 2. Processing Assets ---")
    assets_dir = os.path.join(config.BUILD_DIR, 'assets')
    os.makedirs(assets_dir, exist_ok=True)
    
    # å¤åˆ¶é™æ€èµ„æº
    if os.path.exists(config.STATIC_DIR):
        shutil.copytree(config.STATIC_DIR, config.STATIC_OUTPUT_DIR, dirs_exist_ok=True)
    
    # å¤„ç† CSS
    css_source_path = 'assets/style.css'
    if os.path.exists(css_source_path):
        css_hash = hash_file(css_source_path)
        new_css_filename = f"style.{css_hash}.css"
        config.CSS_FILENAME = new_css_filename # æ›´æ–°é…ç½®
        
        shutil.copy2(css_source_path, os.path.join(assets_dir, new_css_filename))
        print(f"   [Asset] CSS hashed: {new_css_filename}")
    else:
        config.CSS_FILENAME = 'style.css'
        print("   [Warn] style.css not found, using default name.")

    # -------------------------------------------------------------
    # 3. è§£æ Markdown (Core Logic)
    # -------------------------------------------------------------
    print("\n--- 3. Parsing Markdown ---")
    
    md_files = glob.glob(os.path.join(config.MARKDOWN_DIR, '*.md'))
    if not md_files: md_files = glob.glob('*.md') # å…¼å®¹æ¨¡å¼
    
    parsed_posts: List[Dict[str, Any]] = []
    tag_map = defaultdict(list)
    
    for md_file in md_files:
        metadata, content_md, content_html, toc_html = get_metadata_and_content(md_file)
        
        # è¿‡æ»¤éšè—æ–‡ç« 
        if metadata.get('hidden') is True: continue
        # è¿‡æ»¤æ— æ•ˆæ–‡ç« 
        if not all(k in metadata for k in ['date', 'title', 'slug']): continue
            
        post = {
            **metadata, 
            'content_markdown': content_md,
            'content_html': content_html,
            'toc_html': toc_html,
            # ç»Ÿä¸€è·¯å¾„åˆ†éš”ç¬¦ï¼Œé˜²æ­¢ Windows/Linux è·¯å¾„å·®å¼‚
            'link': os.path.join(config.POSTS_DIR_NAME, f"{metadata['slug']}.html").replace('\\', '/')
        }
        
        tag_map_entries = post.get('tags', [])
        for tag_data in tag_map_entries:
            tag_map[tag_data['name']].append(post)
            
        parsed_posts.append(post)

    # æŒ‰æ—¥æœŸæ’åº
    final_parsed_posts = sorted(parsed_posts, key=lambda p: p['date'], reverse=True)
    print(f"   [Parsed] Processed {len(final_parsed_posts)} valid articles.")

    # -------------------------------------------------------------
    # 4. ç”Ÿæˆ HTML (Generation)
    # -------------------------------------------------------------
    print("\n--- 4. Generating Pages ---")
    
    # æ–‡ç« è¯¦æƒ…é¡µ
    for post in final_parsed_posts:
        generator.generate_post_page(post)
    
    # åˆ—è¡¨é¡µ (ä¼ å…¥çš„åˆ—è¡¨ä¸­ç»å¯¹ä¸åŒ…å«å·²åˆ é™¤çš„ MD æ–‡ä»¶)
    generator.generate_index_html(final_parsed_posts)
    generator.generate_archive_html(final_parsed_posts)
    generator.generate_tags_list_html(tag_map)

    # æ ‡ç­¾è¯¦æƒ…é¡µ
    for tag, posts in tag_map.items():
        sorted_tag = sorted(posts, key=lambda p: p['date'], reverse=True)
        generator.generate_tag_page(tag, sorted_tag)

    # ç«™ç‚¹åœ°å›¾ä¸RSS
    generator.generate_robots_txt()
    
    with open(os.path.join(config.BUILD_DIR, config.SITEMAP_FILE), 'w', encoding='utf-8') as f:
        f.write(generator.generate_sitemap(final_parsed_posts))
        
    with open(os.path.join(config.BUILD_DIR, config.RSS_FILE), 'w', encoding='utf-8') as f:
        f.write(generator.generate_rss(final_parsed_posts))
        
    print(f"\nâœ… Build Complete! Output directory: {config.BUILD_DIR}/")
    print("========================================\n")


if __name__ == '__main__':
    build_site()
